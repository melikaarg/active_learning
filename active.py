import numpy as np
import pandas as pd
from statistics import mean
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split


# split dataset into test set, train set and unlabel pool
def split(dataset, train_size, test_size):
	x = dataset[:, :-1]
	y = dataset[:, -1]
	x_train, x_pool, y_train, y_pool = train_test_split(
		x, y, train_size = train_size)
	unlabel, x_test, label, y_test = train_test_split(
		x_pool, y_pool, test_size = test_size)
	return x_train, y_train, x_test, y_test, unlabel, label


if __name__ == '__main__':
	# read dataset
	dataset = pd.read_csv("./spambase.data").values[:, ]

	# imputing missing data
	imputer = SimpleImputer(missing_values = 0, strategy ="mean")
	imputer = imputer.fit(dataset[:, :-1])
	dataset[:, :-1] = imputer.transform(dataset[:, :-1])

	# feature scaling
	sc = StandardScaler()
	dataset[:, :-1] = sc.fit_transform(dataset[:, :-1])

	# run both models 100 times and take the average of their accuracy
	ac1, ac2 = [], [] # arrays to store accuracy of different models

	for i in range(100):
		# split dataset into train(5 %), test(25 %), unlabel(70 %)
		x_train, y_train, x_test, y_test, unlabel, label = split(
			dataset, 0.05, 0.25)

		# train model by active learning
		for i in range(5):
			classifier1 = LogisticRegression()
			classifier1.fit(x_train, y_train)
			y_probab = classifier1.predict_proba(unlabel)[:, 0]
			p = 0.47 # range of uncertanity 0.47 to 0.53
			uncrt_pt_ind = []
			for i in range(unlabel.shape[0]):
				if(y_probab[i] >= p and y_probab[i] <= 1-p):
					uncrt_pt_ind.append(i)
			x_train = np.append(unlabel[uncrt_pt_ind, :], x_train, axis = 0)
			y_train = np.append(label[uncrt_pt_ind], y_train)
			unlabel = np.delete(unlabel, uncrt_pt_ind, axis = 0)
			label = np.delete(label, uncrt_pt_ind)
		classifier2 = LogisticRegression()
		classifier2.fit(x_train, y_train)
		ac1.append(classifier2.score(x_test, y_test))

		''' split dataset into train(same as generated by our model),
		test(25 %), unlabel(rest) '''
		train_size = x_train.shape[0]/dataset.shape[0]
		x_train, y_train, x_test, y_test, unlabel, label = split(
			dataset, train_size, 0.25)

		# train model without active learning
		classifier3 = LogisticRegression()
		classifier3.fit(x_train, y_train)
		ac2.append(classifier3.score(x_test, y_test))

	print("Accuracy by active model :", mean(ac1)*100)
	print("Accuracy by random sampling :", mean(ac2)*100)

'''
This code is contributed by Raghav Dalmia
https://github.com / raghav-dalmia
'''
