{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.100e-01 2.800e-01 5.000e-01 ... 1.010e+02 1.028e+03 1.000e+00]\n",
      " [6.000e-02 0.000e+00 7.100e-01 ... 4.850e+02 2.259e+03 1.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 4.000e+01 1.910e+02 1.000e+00]\n",
      " ...\n",
      " [3.000e-01 0.000e+00 3.000e-01 ... 6.000e+00 1.180e+02 0.000e+00]\n",
      " [9.600e-01 0.000e+00 0.000e+00 ... 5.000e+00 7.800e+01 0.000e+00]\n",
      " [0.000e+00 0.000e+00 6.500e-01 ... 5.000e+00 4.000e+01 0.000e+00]]\n",
      "[[2.10000000e-01 2.80000000e-01 5.00000000e-01 ... 1.01000000e+02\n",
      "  1.02800000e+03 1.00000000e+00]\n",
      " [6.00000000e-02 1.09190635e+00 7.10000000e-01 ... 4.85000000e+02\n",
      "  2.25900000e+03 1.00000000e+00]\n",
      " [4.56837607e-01 1.09190635e+00 6.83974563e-01 ... 4.00000000e+01\n",
      "  1.91000000e+02 1.00000000e+00]\n",
      " ...\n",
      " [3.00000000e-01 1.09190635e+00 3.00000000e-01 ... 6.00000000e+00\n",
      "  1.18000000e+02 0.00000000e+00]\n",
      " [9.60000000e-01 1.09190635e+00 6.83974563e-01 ... 5.00000000e+00\n",
      "  7.80000000e+01 0.00000000e+00]\n",
      " [4.56837607e-01 1.09190635e+00 6.50000000e-01 ... 5.00000000e+00\n",
      "  4.00000000e+01 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./spambase.data\").values[:, ]\n",
    "print(dataset)\n",
    "imputer = SimpleImputer(missing_values = 0, strategy =\"mean\")\n",
    "imputer = imputer.fit(dataset[:, :-1])\n",
    "dataset[:, :-1] = imputer.transform(dataset[:, :-1])\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.03934865 -0.66774627 -0.49003683 ...  0.2505455   1.22818869\n",
      "   1.        ]\n",
      " [-1.67094729  0.          0.06932166 ...  2.22087495  3.25837649\n",
      "   1.        ]\n",
      " [ 0.          0.          0.         ... -0.06244954 -0.15220708\n",
      "   1.        ]\n",
      " ...\n",
      " [-0.66038946  0.         -1.02275921 ... -0.23690579 -0.27260002\n",
      "   0.        ]\n",
      " [ 2.11864456  0.          0.         ... -0.24203686 -0.33856875\n",
      "   0.        ]\n",
      " [ 0.          0.         -0.09049505 ... -0.24203686 -0.40123905\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "dataset[:, :-1] = sc.fit_transform(dataset[:, :-1])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into test set, train set and unlabel pool\n",
    "def split(dataset, train_size, test_size):\n",
    "\tx = dataset[:, :-1]\n",
    "\ty = dataset[:, -1]\n",
    "\tx_train, x_pool, y_train, y_pool = train_test_split(\n",
    "\t\tx, y, train_size = train_size)\n",
    "\tunlabel, x_test, label, y_test = train_test_split(\n",
    "\t\tx_pool, y_pool, test_size = test_size)\n",
    "\treturn x_train, y_train, x_test, y_test, unlabel, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac1, ac2 = [], [] # arrays to store accuracy of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3277\n",
      "0.5049463260084431\n",
      "3276\n",
      "0.47298371383178595\n",
      "3275\n",
      "0.5293022724948719\n",
      "3274\n",
      "0.5198400496723155\n",
      "3273\n",
      "0.5186072093702851\n"
     ]
    }
   ],
   "source": [
    "# split dataset into train(5 %), test(25 %), unlabel(70 %)\n",
    "x_train, y_train, x_test, y_test, unlabel, label = split(\n",
    "\tdataset, 0.05, 0.25)\n",
    "# train model by active learning\n",
    "for i in range(5):\n",
    "\tclassifier1 = LogisticRegression()\n",
    "\tclassifier1.fit(x_train, y_train)\n",
    "\ty_probab = classifier1.predict_proba(unlabel)[:, 0]\n",
    "\tp = 0.47 # range of uncertanity 0.47 to 0.53\n",
    "\tuncrt_pt_ind = []\n",
    "\tprint(unlabel.shape[0])\n",
    "\tfor i in range(unlabel.shape[0]):\n",
    "\t\tif(y_probab[i] >= p and y_probab[i] <= 1-p):\n",
    "\t\t\tprint(y_probab[i])\n",
    "\t\t\tuncrt_pt_ind.append(i)\n",
    "\t\t\tbreak\n",
    "\t# print(uncrt_pt_ind)\n",
    "\tx_train = np.append(unlabel[uncrt_pt_ind, :], x_train, axis = 0)\n",
    "\ty_train = np.append(label[uncrt_pt_ind], y_train)\n",
    "\tunlabel = np.delete(unlabel, uncrt_pt_ind, axis = 0)\n",
    "\tlabel = np.delete(label, uncrt_pt_ind)\n",
    "\n",
    "classifier2 = LogisticRegression()\n",
    "classifier2.fit(x_train, y_train)\n",
    "ac1.append(classifier2.score(x_test, y_test))\n",
    "\n",
    "\n",
    "''' split dataset into train(same as generated by our model),\n",
    "test(25 %), unlabel(rest) '''\n",
    "train_size = x_train.shape[0]/dataset.shape[0]\n",
    "x_train, y_train, x_test, y_test, unlabel, label = split(\n",
    "\tdataset, train_size, 0.25)\n",
    "# train model without active learning\n",
    "classifier3 = LogisticRegression()\n",
    "classifier3.fit(x_train, y_train)\n",
    "ac2.append(classifier3.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1, y_train_1, x_test_1, y_test_1, unlabel_1, label_1 = split(\n",
    "\tdataset, 0.05, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 36, 40, 55, 96, 110, 199, 266, 350, 391, 437, 439, 530, 578, 676, 685, 695, 697, 706, 723, 789, 795, 842, 911, 912, 931, 938, 960, 973, 979, 982, 1089, 1111, 1132, 1219, 1221, 1231, 1272, 1278, 1316, 1319, 1338, 1373, 1424, 1429, 1455, 1483, 1506, 1514, 1544, 1576, 1590, 1608, 1678, 1748, 1817, 1847, 1869, 1873, 1897, 1936, 1990, 1992, 2018, 2033, 2041, 2042, 2055, 2064, 2065, 2085, 2101, 2137, 2189, 2249, 2305, 2318, 2327, 2342, 2506, 2522, 2533, 2579, 2617, 2618, 2627, 2646, 2664, 2691, 2760, 2772, 2834, 2863, 2975, 2977, 2984, 3001, 3012, 3080, 3122, 3136, 3160, 3208, 3251, 3257, 3261]\n",
      "[4, 110, 153, 195, 223, 265, 273, 297, 308, 338, 360, 366, 432, 433, 504, 534, 615, 652, 712, 792, 838, 849, 851, 858, 877, 880, 887, 947, 997, 1015, 1017, 1033, 1047, 1079, 1103, 1132, 1142, 1165, 1167, 1170, 1260, 1268, 1402, 1419, 1447, 1512, 1513, 1596, 1602, 1636, 1759, 1800, 1825, 1837, 1864, 1933, 2065, 2075, 2156, 2204, 2342, 2361, 2401, 2445, 2483, 2485, 2493, 2517, 2572, 2748, 2753, 2785, 2882]\n",
      "[2, 128, 158, 241, 324, 399, 455, 484, 546, 548, 638, 682, 685, 737, 819, 880, 1094, 1141, 1281, 1371, 1451, 1472, 1508, 1567, 1584, 1594, 1645, 1674, 1795, 1877, 1895, 1934, 2050, 2069, 2104, 2142, 2281, 2333, 2402, 2545, 2573, 2665, 2667, 2718, 2842, 2851, 2904, 3070]\n",
      "[105, 124, 198, 394, 434, 479, 543, 560, 654, 671, 766, 1094, 1128, 1212, 1343, 1356, 1365, 1469, 1543, 1603, 1617, 1670, 1979, 2041, 2088, 2243, 2577, 2596, 2878, 2983]\n",
      "[25, 240, 408, 512, 682, 728, 902, 920, 961, 1512, 1529, 1606, 1711, 1956, 1961, 1978, 2288, 2398, 2883]\n",
      "[175, 176, 218, 289, 344, 360, 379, 406, 433, 459, 461, 497, 555, 589, 626, 656, 686, 689, 790, 809, 810, 908, 1014, 1052, 1058, 1109, 1195, 1219, 1225, 1248, 1371, 1377, 1410, 1449, 1455, 1478, 1527, 1543, 1600, 1678, 1693, 1705, 1712, 1718, 1771, 1807, 1829, 1893, 2022, 2054, 2065, 2093, 2100, 2107, 2209, 2211, 2212, 2285, 2329, 2383, 2394, 2410, 2473, 2477, 2561, 2634, 2664, 2705, 2733, 2785, 2789, 2791, 2808, 2815, 2825, 2940, 2962, 2997, 3000, 3006, 3038, 3048, 3135, 3154, 3155, 3214]\n",
      "[20, 88, 169, 270, 277, 294, 380, 403, 421, 458, 464, 494, 521, 540, 592, 671, 678, 764, 787, 795, 803, 839, 925, 942, 968, 1000, 1013, 1094, 1111, 1112, 1151, 1193, 1201, 1212, 1233, 1235, 1238, 1270, 1323, 1366, 1471, 1510, 1538, 1641, 1732, 1756, 1782, 1785, 1826, 1828, 1848, 1876, 1923, 1943, 1953, 1960, 2071, 2088, 2150, 2153, 2223, 2296, 2313, 2463, 2478, 2509, 2557, 2583, 2633, 2777, 2820, 2842, 2926, 2950, 2963, 3044, 3058, 3091, 3106, 3124, 3125, 3140, 3176]\n",
      "[36, 53, 63, 69, 108, 178, 273, 274, 314, 392, 547, 553, 615, 646, 661, 678, 694, 747, 780, 789, 823, 953, 994, 1188, 1341, 1470, 1497, 1551, 1619, 1661, 1864, 2086, 2302, 2400, 2424, 2684, 2693, 2785, 2818, 2899, 2915, 2980, 3054, 3063, 3071, 3093]\n",
      "[1, 129, 204, 215, 346, 364, 386, 452, 526, 533, 595, 660, 692, 789, 899, 972, 1028, 1046, 1078, 1134, 1289, 1306, 1320, 1393, 1394, 1397, 1402, 1430, 1465, 1466, 1571, 1572, 1700, 1942, 1989, 1995, 2061, 2285, 2292, 2338, 2339, 2385, 2509, 2558, 2559, 2637, 2789, 2805, 2928, 3005, 3021, 3042]\n",
      "[43, 152, 184, 247, 295, 297, 323, 342, 368, 562, 772, 831, 872, 952, 1138, 1146, 1205, 1412, 1415, 1439, 1475, 1572, 1675, 1766, 1807, 1810, 1818, 1859, 1869, 1884, 1929, 1943, 1994, 2000, 2008, 2283, 2440, 2591, 2701, 2716, 2734, 2742, 2816, 2911]\n",
      "[35, 51, 64, 102, 126, 152, 164, 190, 211, 216, 219, 271, 334, 391, 412, 460, 499, 513, 515, 523, 525, 526, 554, 556, 564, 590, 591, 622, 646, 701, 732, 765, 777, 784, 829, 903, 907, 931, 943, 979, 1073, 1087, 1090, 1096, 1098, 1158, 1166, 1233, 1243, 1276, 1358, 1377, 1456, 1476, 1495, 1557, 1593, 1612, 1620, 1634, 1672, 1704, 1708, 1732, 1781, 1790, 1819, 1928, 1932, 1934, 2036, 2065, 2106, 2110, 2126, 2142, 2204, 2254, 2284, 2368, 2440, 2452, 2487, 2588, 2640, 2668, 2681, 2689, 2712, 2767, 2770, 2794, 2846, 2854, 2890, 2922, 2923, 2928, 2950, 2952, 2977, 2982, 2993, 3003, 3015, 3020, 3037, 3046, 3075, 3088, 3105, 3117, 3139, 3146, 3151, 3153, 3170, 3179, 3189, 3226, 3234, 3236, 3255]\n",
      "[18, 70, 72, 76, 94, 112, 135, 172, 214, 426, 432, 468, 500, 511, 558, 628, 651, 662, 666, 669, 676, 688, 757, 774, 917, 946, 975, 998, 1071, 1168, 1181, 1248, 1249, 1275, 1290, 1292, 1306, 1313, 1320, 1355, 1359, 1370, 1380, 1426, 1529, 1581, 1588, 1597, 1649, 1682, 1684, 1711, 1736, 1751, 1772, 1799, 1814, 1841, 1863, 1869, 1873, 1875, 1876, 1895, 1989, 2005, 2006, 2207, 2232, 2245, 2281, 2308, 2330, 2382, 2434, 2461, 2487, 2589, 2598, 2634, 2656, 2826, 2863, 2896, 2913, 2942, 2977, 2983, 3092, 3150]\n",
      "[19, 57, 61, 130, 143, 156, 195, 308, 403, 442, 467, 483, 616, 636, 674, 704, 706, 714, 725, 786, 799, 832, 892, 923, 949, 976, 1020, 1076, 1081, 1134, 1188, 1210, 1228, 1229, 1230, 1251, 1264, 1295, 1330, 1381, 1401, 1414, 1416, 1449, 1483, 1549, 1551, 1568, 1657, 1816, 1821, 1823, 1929, 2061, 2073, 2080, 2128, 2205, 2207, 2210, 2300, 2307, 2395, 2455, 2477, 2491, 2651, 2654, 2696, 2698, 2710, 2755, 2807, 2883, 2896, 2915, 2953, 2975, 3037]\n",
      "[4, 50, 83, 88, 139, 148, 231, 235, 254, 287, 329, 406, 466, 472, 541, 579, 625, 648, 649, 810, 869, 937, 1003, 1008, 1048, 1140, 1256, 1340, 1433, 1519, 1568, 1607, 1714, 1752, 1792, 1797, 2020, 2113, 2247, 2343, 2385, 2427, 2507, 2586, 2667, 2732, 2755, 2841, 2843, 2878, 2888]\n",
      "[266, 585, 662, 672, 702, 756, 806, 888, 892, 970, 1078, 1116, 1126, 1127, 1132, 1181, 1212, 1240, 1418, 1555, 1589, 1625, 1646, 1905, 2040, 2144, 2368, 2452, 2469, 2474, 2480, 2483, 2487, 2522, 2546, 2555, 2603, 2688, 2792, 2882, 2928]\n",
      "[39, 50, 64, 70, 75, 83, 123, 165, 260, 262, 265, 395, 399, 458, 461, 473, 506, 589, 601, 618, 631, 650, 653, 672, 715, 804, 825, 881, 882, 918, 924, 957, 1006, 1030, 1056, 1094, 1125, 1143, 1171, 1175, 1207, 1236, 1255, 1266, 1385, 1390, 1396, 1412, 1444, 1480, 1508, 1537, 1556, 1579, 1588, 1605, 1633, 1652, 1743, 1792, 1801, 1844, 1864, 1868, 1923, 1926, 1945, 1953, 1978, 1980, 1992, 2006, 2008, 2012, 2022, 2036, 2060, 2075, 2103, 2104, 2113, 2117, 2184, 2232, 2270, 2306, 2310, 2391, 2409, 2446, 2492, 2494, 2585, 2604, 2616, 2650, 2708, 2740, 2759, 2762, 2795, 2835, 2863, 2911, 2928, 2975, 3019, 3064, 3089, 3102, 3133, 3168, 3178, 3194, 3198, 3204, 3236]\n",
      "[38, 86, 88, 128, 147, 230, 253, 306, 332, 344, 350, 384, 391, 524, 526, 650, 659, 672, 712, 713, 828, 858, 886, 951, 1011, 1094, 1096, 1142, 1151, 1189, 1241, 1286, 1312, 1333, 1384, 1392, 1434, 1461, 1517, 1539, 1541, 1550, 1654, 1707, 1955, 1966, 1986, 2004, 2080, 2102, 2142, 2152, 2188, 2264, 2338, 2359, 2378, 2401, 2496, 2519, 2549, 2554, 2555, 2563, 2569, 2571, 2589, 2642, 2679, 2815, 2876, 2990, 3008, 3035, 3053, 3100, 3101]\n",
      "[26, 53, 57, 61, 62, 68, 114, 154, 159, 160, 169, 213, 239, 256, 257, 338, 410, 436, 476, 479, 547, 586, 593, 648, 828, 855, 881, 884, 1113, 1233, 1256, 1294, 1310, 1323, 1347, 1362, 1455, 1548, 1549, 1553, 1585, 1605, 1625, 1731, 1742, 1784, 1804, 1813, 1833, 1872, 1967, 1984, 2038, 2046, 2090, 2141, 2147, 2157, 2205, 2208, 2244, 2246, 2270, 2287, 2296, 2433, 2492, 2510, 2515, 2539, 2574, 2587, 2728, 2784, 2826, 2836, 2864, 2900, 2986, 2993]\n",
      "[141, 231, 305, 547, 658, 687, 733, 752, 766, 802, 811, 820, 901, 907, 1070, 1292, 1302, 1510, 1568, 1589, 1626, 1698, 1709, 1784, 1882, 1915, 2101, 2160, 2235, 2300, 2353, 2389, 2498, 2569, 2679, 2829, 2909, 2978]\n",
      "[56, 153, 184, 224, 279, 311, 312, 360, 411, 738, 830, 934, 979, 1007, 1115, 1314, 1407, 1462, 1587, 1650, 1668, 1782, 1826, 1858, 1876, 1893, 1955, 1967, 1999, 2149, 2184, 2364, 2409, 2664, 2729, 2787, 2855]\n",
      "[53, 71, 128, 144, 158, 211, 256, 266, 284, 366, 374, 403, 430, 464, 597, 606, 614, 729, 772, 791, 814, 903, 922, 971, 973, 981, 1001, 1003, 1032, 1118, 1126, 1127, 1180, 1265, 1269, 1305, 1344, 1345, 1346, 1359, 1366, 1398, 1456, 1468, 1510, 1546, 1735, 1801, 1893, 1968, 2113, 2139, 2198, 2226, 2264, 2270, 2320, 2437, 2451, 2457, 2585, 2600, 2603, 2611, 2614, 2686, 2703, 2727, 2766, 2776, 2799, 2837, 2852, 2895, 2911, 3015, 3056, 3186, 3246]\n",
      "[2, 79, 106, 142, 196, 218, 229, 276, 292, 336, 351, 365, 387, 392, 420, 424, 431, 439, 458, 485, 516, 531, 543, 736, 741, 754, 782, 783, 833, 972, 975, 1020, 1071, 1204, 1268, 1274, 1286, 1391, 1423, 1430, 1498, 1529, 1541, 1634, 1636, 1672, 1760, 2103, 2148, 2202, 2245, 2264, 2271, 2422, 2534, 2689, 2725, 2826, 2853, 2912, 2978, 3082]\n",
      "[19, 84, 162, 169, 208, 277, 344, 552, 654, 723, 762, 788, 807, 808, 962, 979, 999, 1122, 1206, 1257, 1321, 1322, 1324, 1431, 1540, 1681, 1726, 1796, 1798, 1808, 2074, 2085, 2113, 2135, 2202, 2217, 2221, 2260, 2278, 2292, 2453, 2613, 2685, 2707, 2719, 2802, 2837, 2852, 2872, 2885, 3004, 3084, 3089, 3122]\n",
      "[13, 25, 189, 226, 264, 282, 283, 303, 329, 337, 355, 464, 525, 527, 607, 608, 669, 692, 699, 715, 743, 804, 924, 959, 965, 976, 1001, 1031, 1062, 1104, 1107, 1137, 1206, 1211, 1312, 1339, 1362, 1485, 1487, 1510, 1575, 1587, 1678, 1776, 1839, 1850, 1934, 2212, 2240, 2243, 2245, 2251, 2291, 2319, 2349, 2434, 2439, 2462, 2499, 2501, 2530, 2644, 2650, 2655, 2659, 2707, 2719, 2844, 2939, 2940, 2984, 3040]\n",
      "[24, 129, 173, 214, 223, 313, 364, 458, 536, 566, 666, 811, 864, 879, 980, 1050, 1247, 1374, 1472, 1595, 1611, 1613, 1626, 1711, 1731, 1789, 1826, 1931, 1953, 1966, 1989, 2043, 2123, 2297, 2367, 2413, 2450, 2484, 2518, 2566, 2584, 2610, 2624, 2636, 2637, 2650, 2660, 2676]\n",
      "[84, 97, 167, 192, 204, 250, 285, 408, 461, 486, 500, 536, 565, 569, 575, 579, 629, 640, 644, 652, 674, 680, 732, 735, 744, 771, 798, 844, 868, 878, 949, 1008, 1024, 1028, 1030, 1085, 1091, 1104, 1145, 1148, 1282, 1325, 1345, 1349, 1356, 1367, 1408, 1421, 1487, 1495, 1496, 1514, 1554, 1562, 1591, 1699, 1763, 1788, 1880, 1892, 1911, 1945, 2032, 2078, 2080, 2084, 2110, 2190, 2253, 2336, 2352, 2397, 2433, 2444, 2518, 2528, 2572, 2695, 2716, 2719, 2750, 2828, 2837, 2840, 2891, 2902, 2932, 2935, 2955, 2971, 2975, 3011, 3027, 3056, 3103, 3152, 3207, 3232]\n",
      "[8, 74, 102, 121, 122, 133, 139, 143, 150, 168, 210, 245, 270, 291, 320, 362, 377, 383, 491, 501, 558, 595, 619, 655, 696, 734, 747, 825, 854, 872, 922, 956, 1024, 1160, 1182, 1219, 1291, 1298, 1331, 1333, 1344, 1392, 1425, 1428, 1553, 1562, 1611, 1628, 1648, 1650, 1678, 1709, 1747, 1755, 1844, 1858, 1936, 1961, 2035, 2129, 2289, 2327, 2329, 2333, 2348, 2350, 2355, 2372, 2387, 2421, 2453, 2507, 2533, 2558, 2580, 2597, 2682, 2775, 2782, 2798, 2830, 2837, 2901, 2907, 3010, 3024, 3027, 3037, 3084, 3092, 3099, 3130]\n",
      "[29, 34, 55, 139, 166, 261, 268, 294, 314, 357, 383, 403, 459, 463, 545, 575, 606, 636, 734, 819, 900, 981, 984, 1094, 1113, 1163, 1271, 1283, 1288, 1306, 1361, 1364, 1432, 1443, 1492, 1545, 1572, 1666, 1702, 1708, 1775, 1895, 1914, 1915, 1935, 1946, 1983, 2024, 2028, 2039, 2064, 2066, 2103, 2106, 2136, 2140, 2165, 2225, 2233, 2236, 2340, 2431, 2520, 2578, 2592, 2615, 2636, 2637, 2643, 2687, 2695, 2732, 2785, 2815, 2866, 2890, 2918, 2973, 3017, 3046]\n",
      "[7, 53, 130, 208, 345, 358, 432, 451, 464, 468, 501, 519, 634, 679, 702, 761, 796, 858, 935, 1027, 1113, 1117, 1145, 1249, 1302, 1329, 1340, 1344, 1350, 1358, 1456, 1533, 1669, 1745, 1790, 1803, 1924, 1964, 1985, 1998, 2059, 2061, 2070, 2104, 2138, 2196, 2240, 2278, 2309, 2312, 2327, 2338, 2380, 2418, 2445, 2513, 2545, 2547, 2554, 2638, 2784, 2839, 2848, 2867, 2908]\n",
      "[172, 195, 296, 331, 469, 513, 540, 681, 688, 788, 948, 1139, 1150, 1267, 1347, 1447, 1461, 1616, 1779, 1930, 1938, 1989, 2054, 2221, 2237, 2284, 2435, 2440, 2674, 2772, 2780, 2794, 2863, 2896, 2911]\n",
      "[65, 80, 113, 125, 197, 299, 330, 332, 349, 380, 383, 389, 402, 435, 452, 483, 564, 576, 590, 605, 635, 640, 655, 677, 692, 707, 728, 749, 767, 775, 784, 819, 885, 900, 933, 953, 1005, 1014, 1018, 1020, 1036, 1085, 1091, 1098, 1151, 1174, 1241, 1360, 1372, 1392, 1399, 1455, 1458, 1486, 1544, 1594, 1632, 1635, 1652, 1662, 1690, 1731, 1750, 1761, 1793, 1827, 1875, 1889, 1915, 1959, 2030, 2066, 2128, 2131, 2159, 2165, 2185, 2189, 2248, 2319, 2426, 2434, 2448, 2456, 2459, 2478, 2505, 2575, 2609, 2698, 2711, 2716, 2725, 2756, 2771, 2820, 2823, 2828, 2851, 2892, 2913, 2962, 2986, 3032, 3085, 3148, 3195, 3221, 3261]\n",
      "[33, 183, 199, 205, 255, 283, 291, 465, 524, 559, 599, 602, 609, 632, 713, 847, 848, 854, 943, 1017, 1031, 1057, 1119, 1142, 1157, 1194, 1224, 1291, 1313, 1339, 1349, 1371, 1438, 1468, 1492, 1592, 1677, 1764, 1825, 1831, 1852, 1857, 1918, 1969, 2014, 2049, 2050, 2053, 2094, 2126, 2129, 2134, 2137, 2161, 2196, 2198, 2235, 2415, 2537, 2553, 2595, 2721, 2761, 2911, 2969]\n",
      "[0, 171, 177, 182, 217, 238, 257, 271, 311, 330, 358, 382, 434, 452, 460, 546, 561, 591, 606, 699, 845, 856, 857, 1020, 1095, 1115, 1128, 1145, 1158, 1162, 1178, 1182, 1220, 1229, 1370, 1487, 1494, 1498, 1502, 1548, 1714, 1729, 1769, 1816, 1866, 1904, 1925, 2025, 2037, 2060, 2089, 2090, 2121, 2158, 2190, 2202, 2295, 2325, 2340, 2346, 2350, 2372, 2378, 2397, 2404, 2491, 2663, 2699, 2861, 2872, 2896, 2902, 3063]\n",
      "[2, 102, 109, 219, 312, 359, 394, 501, 778, 794, 918, 922, 951, 1039, 1152, 1222, 1278, 1318, 1320, 1323, 1402, 1458, 1503, 1519, 1601, 1623, 1654, 1658, 1688, 1715, 1788, 1848, 1928, 1971, 1977, 1983, 2051, 2052, 2328, 2437, 2529, 2543, 2550, 2555, 2565, 2609, 2617, 2631, 2639, 2648, 2738, 2859, 2955, 3021]\n",
      "[17, 18, 182, 211, 294, 297, 348, 407, 513, 519, 531, 571, 604, 688, 700, 727, 803, 837, 897, 952, 957, 988, 1079, 1189, 1403, 1526, 1609, 1658, 1660, 1679, 1873, 1964, 2013, 2027, 2037, 2093, 2130, 2134, 2156, 2249, 2264, 2313, 2527, 2539, 2583, 2603, 2681, 2702, 2772, 2773, 2854, 2866, 2947]\n",
      "[48, 72, 114, 123, 172, 278, 292, 319, 345, 382, 423, 434, 436, 479, 486, 562, 563, 650, 747, 882, 885, 892, 893, 937, 990, 1014, 1054, 1065, 1111, 1132, 1138, 1143, 1242, 1270, 1289, 1300, 1364, 1372, 1393, 1402, 1413, 1416, 1488, 1493, 1564, 1568, 1706, 1708, 1805, 1809, 1817, 1832, 1847, 1871, 1896, 1923, 1937, 2040, 2053, 2079, 2122, 2157, 2244, 2307, 2341, 2351, 2393, 2414, 2466, 2492, 2524, 2579, 2632, 2645, 2667, 2671, 2702, 2726, 2730, 2815, 2874, 2956, 3024, 3092, 3144, 3145, 3202, 3211, 3230, 3235, 3274]\n",
      "[35, 41, 67, 73, 98, 114, 126, 138, 176, 207, 329, 354, 495, 612, 653, 844, 845, 846, 911, 917, 934, 968, 984, 1006, 1119, 1143, 1213, 1287, 1370, 1378, 1422, 1440, 1526, 1584, 1631, 1655, 1690, 1732, 1772, 1810, 1917, 1950, 1957, 2018, 2102, 2128, 2176, 2194, 2200, 2245, 2258, 2268, 2334, 2377, 2403, 2415, 2499, 2500, 2505, 2602, 2605, 2724, 2807, 2863, 3034, 3043, 3118, 3129, 3175]\n",
      "[41, 42, 43, 73, 86, 97, 117, 228, 341, 371, 444, 488, 549, 590, 608, 655, 659, 672, 689, 872, 1092, 1241, 1268, 1273, 1327, 1587, 1680, 1762, 1768, 1787, 1811, 1848, 1909, 1950, 2005, 2065, 2289, 2349, 2350, 2428, 2452, 2504, 2553, 2685, 2753, 2800, 2858, 2873, 2880, 2939, 2952, 2957, 2965, 3001]\n",
      "[29, 76, 306, 404, 459, 617, 862, 865, 883, 888, 971, 1014, 1032, 1127, 1313, 1338, 1631, 1671, 1891, 1916, 1995, 2095, 2349, 2363, 2506, 2579, 2616, 2629, 2630, 2817, 2846, 3045]\n",
      "[73, 83, 166, 277, 339, 353, 373, 388, 510, 772, 822, 1038, 1137, 1153, 1260, 1432, 1619, 2294, 2599, 2993, 3027]\n",
      "[134, 164, 173, 208, 227, 234, 262, 273, 303, 430, 434, 446, 457, 519, 547, 578, 589, 594, 614, 626, 627, 672, 722, 756, 761, 768, 774, 790, 797, 804, 825, 916, 926, 944, 976, 1001, 1018, 1045, 1117, 1140, 1143, 1194, 1212, 1237, 1263, 1268, 1299, 1392, 1440, 1477, 1506, 1526, 1528, 1543, 1563, 1566, 1574, 1585, 1591, 1638, 1645, 1661, 1814, 1859, 1904, 1905, 1920, 1922, 1984, 1985, 2002, 2009, 2017, 2030, 2078, 2123, 2166, 2179, 2191, 2206, 2224, 2228, 2253, 2257, 2279, 2280, 2315, 2349, 2429, 2445, 2471, 2475, 2476, 2553, 2583, 2635, 2641, 2667, 2671, 2712, 2713, 2719, 2763, 2770, 2798, 2814, 2816, 2881, 2952, 2953, 2961, 2966, 2986, 3011, 3013, 3024, 3029, 3049, 3061, 3092, 3103, 3105, 3144, 3175, 3193, 3212, 3254, 3264, 3273]\n",
      "[168, 229, 340, 492, 519, 543, 602, 718, 720, 752, 781, 901, 986, 1006, 1037, 1053, 1067, 1081, 1085, 1105, 1114, 1124, 1153, 1234, 1237, 1238, 1331, 1335, 1412, 1424, 1436, 1439, 1450, 1467, 1485, 1518, 1519, 1538, 1558, 1758, 1841, 1875, 1889, 1924, 1939, 1950, 1963, 1967, 1996, 2049, 2062, 2098, 2139, 2175, 2233, 2254, 2273, 2316, 2379, 2411, 2445, 2459, 2490, 2533, 2598, 2656, 2688, 2700, 2744, 2746, 2747, 2765, 2811, 2826, 2857, 2931, 2987, 2999, 3034, 3042, 3056, 3130, 3136]\n",
      "[57, 95, 98, 105, 112, 136, 186, 212, 321, 322, 365, 394, 421, 476, 478, 496, 555, 618, 684, 711, 766, 769, 823, 874, 931, 939, 1007, 1016, 1091, 1121, 1133, 1182, 1259, 1278, 1292, 1341, 1364, 1423, 1440, 1451, 1497, 1501, 1567, 1602, 1621, 1623, 1625, 1628, 1763, 1829, 1925, 1936, 2046, 2075, 2086, 2103, 2172, 2377, 2430, 2498, 2561, 2595, 2599, 2715, 2720, 2771, 2774, 2922, 2980]\n",
      "[33, 96, 142, 159, 184, 239, 246, 362, 413, 559, 606, 608, 759, 960, 971, 972, 1126, 1150, 1195, 1321, 1374, 1404, 1433, 1449, 1539, 1682, 1767, 1775, 1816, 1866, 1915, 1944, 2113, 2162, 2164, 2418, 2675, 2772, 2840, 2857]\n",
      "[0, 191, 262, 305, 599, 703, 816, 1020, 1022, 1090, 1119, 1120, 1209, 1407, 1608, 1642, 1855, 1993, 2107, 2354, 2699, 2954]\n",
      "[15, 58, 59, 68, 82, 108, 138, 185, 197, 241, 246, 251, 298, 304, 331, 335, 358, 424, 488, 502, 504, 512, 526, 527, 548, 565, 628, 736, 792, 821, 833, 890, 949, 1050, 1123, 1124, 1167, 1179, 1197, 1237, 1242, 1257, 1281, 1282, 1287, 1300, 1303, 1307, 1319, 1425, 1484, 1515, 1518, 1547, 1575, 1610, 1667, 1674, 1699, 1718, 1789, 1803, 1832, 1883, 1895, 1908, 1915, 1918, 1923, 1989, 2017, 2058, 2122, 2132, 2134, 2149, 2211, 2214, 2274, 2280, 2354, 2355, 2424, 2553, 2580, 2597, 2605, 2630, 2645, 2670, 2686, 2693, 2729, 2735, 2741, 2758, 2856, 2948, 2962, 3096, 3168, 3170, 3214, 3232]\n",
      "[5, 116, 176, 286, 307, 346, 357, 442, 463, 521, 549, 637, 676, 715, 724, 727, 802, 821, 852, 882, 890, 917, 932, 981, 982, 1014, 1081, 1110, 1116, 1143, 1213, 1275, 1297, 1298, 1353, 1369, 1425, 1431, 1465, 1503, 1513, 1515, 1640, 1692, 1698, 1740, 1805, 1839, 1948, 1985, 2023, 2054, 2072, 2084, 2121, 2125, 2167, 2172, 2176, 2185, 2188, 2196, 2359, 2400, 2408, 2428, 2436, 2472, 2489, 2567, 2569, 2653, 2690, 2753, 2761, 2832, 2836, 2894, 2917, 2931, 2951, 2968, 3001, 3041, 3074, 3081]\n",
      "[25, 26, 46, 100, 130, 150, 167, 195, 219, 242, 358, 400, 412, 516, 536, 635, 656, 776, 808, 817, 888, 941, 987, 1071, 1137, 1201, 1323, 1387, 1466, 1476, 1502, 1717, 1827, 1870, 1880, 1963, 1987, 1996, 2034, 2038, 2041, 2043, 2145, 2146, 2245, 2264, 2304, 2334, 2395, 2457, 2575, 2588, 2796, 2888, 2956, 2976, 2979, 3074]\n",
      "[5, 43, 160, 204, 231, 244, 270, 280, 354, 409, 476, 583, 592, 639, 682, 717, 746, 763, 813, 877, 991, 1014, 1040, 1148, 1193, 1272, 1274, 1445, 1483, 1658, 1795, 1911, 1939, 1949, 1951, 1986, 2025, 2223, 2254, 2286, 2295, 2331, 2342, 2359, 2448, 2503, 2507, 2597, 2636, 2803, 2914, 2999, 3025]\n",
      "[69, 149, 236, 313, 499, 956, 1120, 1303, 1342, 1350, 1507, 1523, 1614, 1646, 1672, 1887, 1984, 2002, 2103, 2123, 2149, 2230, 2432, 2520, 2703, 2759, 2783, 2787, 2841]\n",
      "[0, 12, 78, 110, 234, 261, 280, 293, 324, 359, 397, 399, 403, 453, 493, 505, 552, 581, 622, 654, 667, 769, 776, 849, 942, 988, 1010, 1029, 1051, 1073, 1093, 1183, 1198, 1225, 1253, 1265, 1301, 1312, 1430, 1465, 1505, 1519, 1526, 1592, 1597, 1632, 1638, 1642, 1643, 1644, 1723, 1774, 1781, 1842, 1879, 1880, 1917, 2066, 2142, 2179, 2237, 2244, 2251, 2267, 2295, 2297, 2339, 2395, 2425, 2455, 2457, 2512, 2521, 2569, 2570, 2581, 2619, 2626, 2647, 2657, 2682, 2748, 2751, 2774, 2837, 2855, 2991, 3001, 3041, 3186, 3202, 3221, 3253, 3257]\n",
      "[40, 55, 88, 119, 137, 149, 150, 191, 242, 302, 454, 459, 491, 543, 563, 569, 602, 613, 683, 705, 752, 768, 797, 807, 907, 917, 948, 1065, 1080, 1083, 1084, 1138, 1177, 1183, 1374, 1384, 1447, 1464, 1468, 1502, 1506, 1518, 1519, 1544, 1626, 1689, 1731, 1733, 1802, 1870, 2007, 2085, 2133, 2135, 2298, 2302, 2345, 2356, 2364, 2381, 2400, 2410, 2451, 2468, 2476, 2507, 2570, 2594, 2612, 2620, 2678, 2739, 2887, 2929, 2951, 2964, 2974, 2997, 3051, 3080, 3139]\n",
      "[72, 92, 104, 111, 157, 173, 324, 327, 401, 543, 573, 663, 810, 827, 836, 866, 878, 916, 920, 944, 1012, 1049, 1052, 1136, 1172, 1179, 1211, 1221, 1257, 1298, 1419, 1424, 1477, 1510, 1589, 1704, 1761, 1846, 1876, 1934, 1971, 2051, 2060, 2131, 2206, 2256, 2347, 2524, 2582, 2598, 2642, 2882, 2900, 3005, 3020]\n",
      "[67, 69, 121, 220, 392, 402, 408, 452, 490, 634, 704, 817, 819, 927, 1083, 1148, 1149, 1450, 1723, 1766, 1849, 1912, 1975, 2246, 2367, 2471, 2498, 2523, 2559, 2627, 2763, 2791, 2795, 2818, 2890, 2907, 3044]\n",
      "[52, 129, 247, 295, 360, 460, 470, 723, 724, 761, 776, 804, 1006, 1088, 1522, 1546, 1619, 1831, 1954, 2003, 2236, 2385, 2392, 2414, 2510, 2672, 2767, 2851, 2860, 3001]\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_29490/2850622492.py\", line 21, in <module>\n",
      "    classifier2.fit(x_train, y_train)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 117, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 450, in _logistic_regression_path\n",
      "    opt_res = optimize.minimize(\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/scipy/optimize/_minimize.py\", line 699, in minimize\n",
      "    res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/scipy/optimize/_lbfgsb_py.py\", line 360, in _minimize_lbfgsb\n",
      "    f, g = func_and_grad(x)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 285, in fun_and_grad\n",
      "    self._update_fun()\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 251, in _update_fun\n",
      "    self._update_fun_impl()\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 155, in update_fun\n",
      "    self.f = fun_wrapped(self.x)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\", line 137, in fun_wrapped\n",
      "    fx = fun(np.copy(x), *args)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/scipy/optimize/_optimize.py\", line 76, in __call__\n",
      "    self._compute_if_needed(x, *args)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/scipy/optimize/_optimize.py\", line 70, in _compute_if_needed\n",
      "    fg = self.fun(x, *args)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/sklearn/linear_model/_linear_loss.py\", line 289, in loss_gradient\n",
      "    grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2052, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/stack_data/core.py\", line 720, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/stack_data/core.py\", line 663, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/stack_data/core.py\", line 600, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/stack_data/core.py\", line 164, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/executing/executing.py\", line 428, in asttext\n",
      "    self._asttext = ASTText(self.text, tree=self.tree, filename=self.filename)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/asttokens/asttokens.py\", line 307, in __init__\n",
      "    super(ASTText, self).__init__(source_text, filename)\n",
      "  File \"/home/melika/.local/lib/python3.8/site-packages/asttokens/asttokens.py\", line 44, in __init__\n",
      "    source_text = six.ensure_text(source_text)\n",
      "AttributeError: module 'six' has no attribute 'ensure_text'\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "\t# split dataset into train(5 %), test(25 %), unlabel(70 %)\n",
    "\tx_train, y_train, x_test, y_test, unlabel, label = split(\n",
    "\t\tdataset, 0.05, 0.25)\n",
    "\t# train model by active learning\n",
    "\tfor i in range(5):\n",
    "\t\tclassifier1 = LogisticRegression()\n",
    "\t\tclassifier1.fit(x_train, y_train)\n",
    "\t\ty_probab = classifier1.predict_proba(unlabel)[:, 0]\n",
    "\t\tp = 0.47 # range of uncertanity 0.47 to 0.53\n",
    "\t\tuncrt_pt_ind = []\n",
    "\t\tfor i in range(unlabel.shape[0]):\n",
    "\t\t\tif(y_probab[i] >= p and y_probab[i] <= 1-p):\n",
    "\t\t\t\tuncrt_pt_ind.append(i)\n",
    "\t\tprint(un)\n",
    "\t\tx_train = np.append(unlabel[uncrt_pt_ind, :], x_train, axis = 0)\n",
    "\t\ty_train = np.append(label[uncrt_pt_ind], y_train)\n",
    "\t\tunlabel = np.delete(unlabel, uncrt_pt_ind, axis = 0)\n",
    "\t\tlabel = np.delete(label, uncrt_pt_ind)\n",
    "\tclassifier2 = LogisticRegression()\n",
    "\tclassifier2.fit(x_train, y_train)\n",
    "\tac1.append(classifier2.score(x_test, y_test))\n",
    "\t''' split dataset into train(same as generated by our model),\n",
    "\ttest(25 %), unlabel(rest) '''\n",
    "\ttrain_size = x_train.shape[0]/dataset.shape[0]\n",
    "\tx_train, y_train, x_test, y_test, unlabel, label = split(\n",
    "\t\tdataset, train_size, 0.25)\n",
    "\t# train model without active learning\n",
    "\tclassifier3 = LogisticRegression()\n",
    "\tclassifier3.fit(x_train, y_train)\n",
    "\tac2.append(classifier3.score(x_test, y_test))\n",
    "\n",
    "print(\"Accuracy by active model :\", mean(ac1)*100)\n",
    "print(\"Accuracy by random sampling :\", mean(ac2)*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
